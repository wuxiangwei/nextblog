---
layout: post
title: 聊聊Raft一致性算法2
date: 2017-12-12
author: wuxiangwei
categories: 算法 Raft
tags: 算法 Raft
---

* Kramdown table of content
{:toc, toc}


**术语**

| 中文   | 英文  | 含义                                                       |
| :--    | :--   | :--                                                        |
| 日志   | Log   | 由日志项构成                                               |
| 日志项 | Entry | 包含index、term、command的一组数据，对应于client的一个请求 |


## 一致性算法 ##

特点：

1. 安全性。在存在网络延迟、网络分区、丢包、重复、乱序的情况下仍然能够返回正确的结果；
2. 可用性。只要多数派能够正常工作，集群就能够正常工作；
3. 不依赖时钟来保证一致性；

## Leader Election ##

Leader选举

1. 触发选举的机制（心跳超时，Follower发起一轮选举）；
2. Server启动时为Follower角色；


发起选举的准备工作：
1. 增加currentTerm；
2. 角色从Follower转换为Candidate；
3. 投票给自己；
4. 并发发送RequestVote RPC给其它Server；

RequestVote RPC:
Arguments：
term  候选人的term
candidateId
lastLogIndex  候选人最近一条log的索引
lastLogTerm   候选人最近一条log的term

Results:
term  currentTerm 用于让候选人更新自己
voteGranted 是否投票给候选人


目标：
1. 给定的term最多只产生1个Leader （多数派规则）
2. Leader拥有最新的Log，选举结束就能提供服务，不需要同步数据。

当选为Leader的条件：收到多数派的投票。
多数派能够保证，对给定的term最多只会产生1个Leader。


投票规则：
1. 1个Term只能投1票；（投多票就会产生多个多数派）
2. 先到先得；
3. 候选人的term不小于currentTerm；
4. 候选人的log至少和自己相同新，或者更新；


CASE1：
候选人赢得选举。
赢得选举后，向其它所有Server发送心跳，避免其它节点超时触发新选举。


CASE2：
选举期间收到其它Server成为Leader的RPC：
1. 如果新Leader的term不小于候选人的currentTerm，承认新Leader的合法性，切换为Follower角色；
2. 如果新Leader的term小于候选人的currentTerm，拒绝RPC，继续呆在Candidate角色。

CASE3：
候选人既没有赢得选举也没有输掉选举。
举个例子：多个Followers同时发起选举，把票都投给了自己，从而无法形成一个多数派。
出现这种情况时，每个候选人都会超时，超时后再次增加term发起新一轮的选举。
每个Server的选举超时时间是给定范围内的一个随机值，这可以降低split vote问题出现的概率。

## Log replication ##

### Log Matching Property ###

> Log Matching Property
> 1. If two entries in different logs have the same index and term, then they
>    store the same command.
> 2. If two entries in different logs have the same index and term, then the
>    logs are identical in all preceding entries.

在任意两个节点，对具有相同index和term的两条日志项，满足下面两个条件：

1. 这两条日志项的内容(command)相同；
2. 这两个节点中，这条日志项前面的所有日志项也一一对应并且相同。

对第一条，首先日志项由Leader创建，对给定的index和term，Leader最多只会创建1条日志项。并且，Leader不会修改日志项的index。
对第二条，Leader发送AppendEntries RPC时会携带前一条日志项的index和term（即prevLogIndex和prevLogTerm），Follower接收RPC后会检查本地是否包含prevLogIndex和prevLogTerm指定的日志项，如果不存在则拒绝RPC。这个过程称为一致性检查，通过这个检查可以推导出第二条。首先，初始情况每个节点都没有日志项，满足Log Matching条件；后面，根据一致性检查来保证条件2。


这个特性保证日志项间不存在空洞。

正常情况，Leader的Logs和所有Follower的Logs保持一致。
不过，在Leader反复故障，多次Leader选举后，各节点的Logs会出现不一致，参考论文的Figure7。

节点f在term为2时当选为Leader，从client接收3条日志项，持久化到本地，但都没复制到其它节点就挂掉了。挂掉后重新开始Leader选举，并且再次当选为Leader，此时term为3，从client接收5条日志项后再次挂掉，挂点后一直没起来，并且term为3时接收的日志项都没有复制到其它节点。

接下来，节点e当选为新Leader，term为4，处理了4条日志项后挂掉，再也没起来。这4条日志项中，前两条复制到多数派，后两条只记录到本地。
接下来，节点d当选为新Leader，term为5，处理了2条日志项后挂掉。不过这两条日志被复制到了多数派。
接下来，节点c当选为新Leader，term为6，处理了4条日志项后挂掉。这4条日志项中，前两条复制到了多数派，第3条只复制到了节点d，最后1条在本地。
接下来，节点d当选为新Leader，term为7，处理了2条日志项后挂掉。不过这两条日志项只在本地，没有复制到其它节点。
接下来，产生term8，就称为了Figure7中的状态。

| Term | Leader | 本地 | 多数派 |
| :--  | :--    | :--  | :--    |
| 2    | f      | 3    | 0      |
| 3    | f      | 5    | 0      |
| 4    | e      | 4    | 2      |
| 5    | d      | 2    | 2      |
| 6    | c      | 4    | 2      |
| 7    | d      | 2    | 0      |

**注**：本地列是指复制到Leader本地的日志项数量，多数派是指复制到多数派的日志项数量。

**问题**：新Leader当选后不会去同步数据，而是直接开始服务？


### 不一致Log的处理 ###

> In Raft, the leader handles inconsistencies by forcing the followers'logs to
> duplicate its own.

Leader强制让Followers的Logs和自己保持一致，对Follower中不一致的日志项，删除。大致做法分两步：首先，从后往前找到第1条一致的日志项，根据Log Matching Property，这条日志项前面的所有日志项都跟Leader保持一致了。对这条日志项后面的日志项，先在Follower中删除，然后从Leader复制。

从具体实现来说，Leader为每个Follower维护1个*nextIndex*索引，Leader当选后先将*nextIndex*初始化为本地最后1条日志项加1。如果Follower的日志和Leader日志不一致，那么它会拒绝AppendEntries RPC，Leader被拒绝后递减对应的*nextIndex*索引，再继续尝试。最后，到达两者一致的那条日志项时，AppendEntry RPC会成功，此时在Follower中删除不一致的日志项，使Leader和Follower的日志一致。


**Tips**

对不一致的日志，前面所述方法，1个AppendEntries RPC只能检查出1条不一致的日志项，效率低。Follower发现不一致时将不一致日志项所在的term一并返回，Leader直接跳过这个term中的所有日志项。这种优化，在复制Leader时可能会重复发送一致的日志项。这时要求RPC满足幂等性，Follower要丢弃已经存在的日志项。


ceph paxos的处理方式

## Safety ##

**目标**：保证每个节点状态机执行命令的顺序相同。

### Leader Completeness Property ###

> If a log entry is committed in a given term, then that entry will be present
> in the logs of the leaders for all higher-numbered terms.

新Leader的Log中包含所有已commit的日志项，这就是 **Leader Completeness Property**。


两种方法可以达到这个目的：一种是新Leader选举出来后开始服务前先向所有Follower同步数据，第二种是Leader选举过程中直接选择一个日志中包含所有已commit日志项的节点。Raft选择第二种方法，Ceph Paxos选择第一种方法。

1条日志项被commit意味着这条日志项已经被复制到了多数派。
1个节点当选为Leader也必须有多数派的支持。也就是说，这个多数派中至少有1个节点包含了最近commit的日志项，而节点投票给Leader的条件是Leader最近commit的日志项要不比本节点的最近commit的日志项旧。所以Leader包含了最近commit的日志项，从而包含所有已commit的日志项。

具体实现来说，RequestVote RPC包含lastLogIndex和lastLongTerm，如果Follower接收到后与自己进行比较，如果对方比较旧，则拒绝投票给它。


参考论文Figure8：

| 阶段 | term | Leader               | 日志项                      |
| :--  | :--  | :--                  | :--                         |
| a    | 2    | S1                   | 2被复制到S1、S2             |
| b    | 3    | S5                   | 3写到S5本地                 |
| c    | 4    | S1                   | 2被复制到S1、S2、S3(new)    |
| d    | 5    | S5（S2、S3、S4投票） | 3被复制到S1、S2、S3、S4、S5 |

问题：在阶段c中，日志项2已经commit（日志项2被复制到多数派），但在阶段d中，日志项2被覆盖删除。
产生问题的原因是，Leader复制旧term的日志项到其它节点。

解决这个问题的方法是，Follower只提交Leader当前Term的日志项。根据 Log Matching Property 当前Term的日志项被提交，它前面的日志项也间接地被提交了。（ *具体实现来说，Follower接收前面term的日志项保持在内存中，但先不写磁盘* ）


**Leader Completeness Property**的证明。

反证法：

假设Leader T在term T内commit了一条日志项，这条日志项在将来的term的新Leader中不存在了，假设不存在这条日志项的最小term为U，U>T。

1. 在Leader U选举阶段，那条已经提交的日志项就不存在了（否则，不会出现Leader U的日志中不存在那条日志的情况）；
2. Leader T将日志项复制到了多数派，Leader U能赢得选举也必须有多数派的支持。那么至少存在这么一个节点，既复制了来自Leader T的日志项，也将选票投给了Leader U。
3. Voter（投票者）必是先复制了来自Leader T的日志项，然后再投票给Leader U。否则，它会拒绝Leader T的AppendEntries RPC，因为Voter的currentTerm会大于T（投票给Leader U的条件是currentTerm不小于U，而U>T，所以currentTerm > T）。
4. Voter在投票给Leader U时仍然持有这条已提交的日志项。因为根据假设中间的Leader都包含这条日志项，而Leader不会删除日志项，Follower只会删除和Leader有冲突的日志项。
5. Voter投票给Leader U，所以Leader U的日志不会比Voter旧。这就开始引出冲突点了。
6. 首先，如果Voter和Leader U的日志中最后一个term相同，那么Leader U的日志至少跟Voter是等长的，也就是说，Leader U包含Voter的所有日志项。 这就有了第一个冲突点，根据假设Leader U不包含那条已提交的日志项，而Voter包含已提交的日志项。

7. 除此之外，如果Leader U的日志的最后一个term大于Voter的最后一个term。那么Leader U的最后一个term也是大于T的，因为Voter的最后一个term不小于T（因为Voter已经复制了Leader T的日志项）。根据假设，Leader U的前一个Leader包含已提交的日志项，根据Log Matching Property，Leader U也必须包含这条日志项。

8. 假设失败，所以，所有大于T的term的Leader都包含所有在term T中提交的日志项，成立。
9. Log Matching Property保证新Leader会包含所有已提交的日志项。


### State Machine Safety Property ###

> If a server has applied a log entry at a given index to its state machine, no
> other server will ever apply a different log entry for the same index.

所有server在相同index中apply的日志项都相同。


## Follower、Candidate 故障处理 ##

相比于Leader故障的情况，Follower和Candidate的故障处理更简单，并且两者的处理方式相同：

1. RPC重试，直到节点重新启动。
2. RPC幂等性，对相同请求的多次处理结果相同。例如处理AppendEntries时对已经存在的日志项直接忽略。

## Timing and Available ##

Raft的Safety不依赖时钟，但可用性依赖时钟，可用性是指系统对client请求的响应。

```
broadcastTime << electionTimeout << MTBF
```

broadcastTime 广播时间， 1台server并发发送RPC给集群中所有server，并且收到回复的平均时间；
electionTimeout 选举的超时时间；
MTBF 平均故障间隔，单台server相邻两次故障的平均时间间隔。

broadcastTime依赖于磁盘性能，因为RPC会要求持久化一些数据到磁盘，范围在0.5ms到20ms之间。
electionTimeout 可以设置为10ms到500ms之间。
MTBF 一般是几个月或者更长时间，所以一定是能够满足要求的。


要求broadcastTime比electionTimeout低一个数量级：
1. Leader可以发送稳定的心跳以阻止Follower启动新一轮的选举；
2. 每个server的electionTimeout是随机的，这个要求可以避免出现split vote问题。

要求electionTimeout比MTBF低一个数量级：
1. Leader故障后，系统将有一段时间是不可用的，这段时间大致是electionTimeout。应该尽力避免这种情况。


## Cluster membership Change ##

集群成员变更。

目标：

1. 不影响安全性，变更过程中故障了仍然能够自动恢复；
2. 配置转换过程中仍然能够响应client的请求；


有两种配置更新方式，一种是离线更新，一种是在线更新。离线更新会导致服务不可用，不可取。在线更新可能出现 **双Leader问题**。如图Figure10，将集群从3个节点增加到5个节点。Server1和Server2使用旧配置选出一个Leader，Server3、Server4和Server5使用新配置选出另一个Leader。

### single-server变更 ###


在一次变更中添加或删除多个server，可能会出现多Leader问题。
出现多Leader的问题是，变更过程中由于配置不同，集群分裂出两个 **独立**的多数派，1个是旧配置的多数派，1个是新配置的多数派。
如果一次变更中只添加或删除1个server，就不会出现两个没有交集独立的多数派。以3个节点增加到5个节点为例，3个节点时多数派为2个，5个节点时多数派为3个，2个加3个恰好为5，这两个多数派见可以没有交集。以3个节点增加1个节点到4个节点为例，3个节点时多数派为2，4个节点时多数派为3，但2个加3个为5个要大于集群总数量4，所以必有1个节点同时属于两个多数派。参考Figure 4.3。

所以，raft限制1次变更中只允许增加或删除1个节点。

### server接收配置entry时立即生效 ###

Cnew entry的几个点：

1. Cnew entry复制给Cnew server；
2. Cnew server的多数派复制entry后，Cnew entry才算被提交；
3. **Cnew server复制Cnew entry后就立即生效，不必等到Cnew entry提交后才生效**；

新配置变更在Cnew entry被提交时完成，Cnew entry的提交意味着：

3. Cnew entry 已经在多数派中生效；
4. 没收到Cnew entry的server在下轮选举中不会成为leader；
1. 此次配置变更已完成，允许开始下个配置变更，此时就可以避免出现两个多数派的问题了；
2. 如果此次变更是剔除一个server，那么这个server可以shut down了。

???
如果server在知道Cnew entry已提交后才生效，那么会存在几个问题：

1. leader需要track哪些server已经知道Cnew entry已提交的消息；???
2. server需要持久化commit index到磁盘；???

这两件事情在现有的raft中都没有要求。

server复制Cnew entry后就立即生效，也会有问题：如果在Cnew 复制到多数派前就发生新一轮选举，那么Cnew entry可能会被删除，那么复制了Cnew entry的server就需要做 **回退**。

1. 如果leader在Cnew配置中不存在，server仍要接收leader的AppendEntries
   RPC。否则，新server将无法接收到Cnew entry前的entries。（leader何时下台??? ）
2. server可以投票给配置中不存在的候选人。考虑如下场景，在3个server的集群中增加1个server。

| A    | B(leader) | C    | D(new) |
| :--  | :--       | :--  | :--    |
| v    | v         | v    | v      |
| Cold | Cnew      | Cnew | Cnew   |
| v    | x         | v    | v      |
| Lold | x         | Lold | Lnew   |

节点B宕机；
节点D有最新的log以及新配置；
节点C有新配置但log不是最新的；
节点A为旧配置（不包含节点D），并且log不是最新的。

此时发生选举，A只要求2个节点就能组成多数派，但由于log是最旧的，没有节点投票给它，所以不能成为leader。
C和D都要求有3个节点才能组成多数派。C节点可以接收到A的投票，以及自己的投票，D不会投票给它（因为日志比较旧），所以只有2票，无法成为leader；
D节点能够接收到C的投票，以及自己的投票，如果A不投票给D，那么D也只有2票，无法成为leader。
所以，此时无法选出1个leader，集群不可用。

解决方法是，A也可以投票给D，也就是说投票给一个不在自己配置中的server，从而形成多数派。

### 对可用性(Availability)的影响 ###


#### 新节点影响新entry提交速度 ####

Figure 4.4

问题描述：

新节点不包含历史log，为满足Log match property，必须先追上leader的log后才能提交最新的entry。这样，如果新entry的提交要依赖新节点，那么新节点将成为瓶颈严重拖延新entry的提交。新entry的提交是否依赖于新节点，关键看多数派是否必须包含新节点。

举个例子，假设集群有3个节点S1、S2、S3，通过配置变更新加了一个节点S4，S4不包含历史log。此时节点S3节点离线，集群仍然可用，此时要求多数派的个数为3，并且此时在线的节点只有3个，也就是说新节点S4必须是多数派的一员。 再举个例子，集群一开始还是3个节点S1、S2、S3，通过3次连续的配置变更，集群成员增加到6个。在有6个节点的情况下，多数派的个数为4个，但集群只有3个旧节点，要形成多数派必须要有1个新节点。


解决方法：

在配置变更前增加一个新阶段，将新节点以non-voting的角色加入集群，先从leader同步数据，数据同步结束后再开始配置变更。 non-voting成员，顾名思义，没有投票权。具体来说，在leader选举阶段没有投票权，以及在entry提交时不能作为多数派的成员。

leader如何知道新节点的log为空？
正常情况，根据AppendEntries的一致性检查，直到nextIndex变为1时才能确定新leader的log为空。这种方式，在添加新节点的情况，会比较影响性能，特别是


Attempting to add a server that is unavilabble or slow is often a mistake.
新节点复制entries能否追赶上leader，是个问题。如果新节点本身不可用，或者新节点非常地慢，leader就放弃变更。


non-voting节点复制到多少entries的时候，才允许开始更新配置加入集群？


#### 新配置中删除的节点为当前的leader ####

leader何时下台？

一种做法是，leader将leadership交接给另外一个server。

一旦Cnew entry被提交，leader就可以下台了。如果leader在Cnew entry提交前下台，那么leader节点可能再次被选举为leader。考虑极端情况，集群从2个节点变更为1个节点。如果Cnew entry还没提交，也就是S2还没复制Cnew entry，此时S2尚无法成为leader，因为根据Cold它需要S1的投票才能成为leader（Cnew中它给自己投票就能成为leader）。


#### 已剔除server扰乱集群 ####

如果Cnew entry是剔除一个server的操作，那么leader创建出Cnew entry后就不再给要剔除的server发送心跳了。被踢的server心跳超时后会递增term新发起一轮选举，leader接收到投票请求后发现自己的term比对方小，就会自动转到Follower角色。从而导致集群暂时没有leader，出现可用性问题。而且，被踢的server一直都接收不到Cnew entry，所以会一直发起选举，扰乱集群。

解决方法是针对这种情况，每个server接收到RequestVote后拒绝更新term以及拒绝投票给对方，即忽略这条投票请求。接下来的问题是，server如何识别出这种情况。在这种情况下leader是合法有效的，它会周期性地给其它server发送心跳，所以server可以通过比较接收到RequestVote时间和最近一次心跳的时间差是否超过eletion timeout来决定是否丢弃投票请求。正常地，如果是leader宕机的情况，那么所有server也几乎是在election timeout后才会发起选举，其它server在election timeout后才会收到RequestVote请求。所以，这种改动不会影响原来的正常流程。

**Pre-Vote algorithm**

如前所述，server直接递增iterm发起一轮选举时会扰乱集群，让leader转换为follower影响集群可用性。
为提高server发起选举后成为leader的概率，在发起RequestVote请求前先通过Pre-Vote请求确定自己有可能成为leader，然后再发起RequestVote请求。自己可能成为leader的条件是：

1. server的log足够新 (不够新就不可能成为leader，没必要发起选举)
2. 投票者已经很长一段时间内没有接收到leader的心跳（解决前文提到扰乱集群的问题）

这样，当server离开集群时不会扰乱集群，当server重新加入集群时也不会扰乱集群。


## Log Compaction ##

日志压缩的目的：

1. 缩短replay的时间；
2. 减少磁盘空间占用；


## Client Interaction ##

目标：

1. client如何找到Leader；
2. 支持线性语义（Linearizable semantics）


### 连接Leader ###

为什么要寻找Leader：

1. 只有Leader才能处理client的请求；
2. Leader是动态选举出来的，所以没法给client指定；

如何寻找Leader：

1. 随机选择1个Server节点；
2. 如果Server节点不是Leader，那么Server会拒绝处理请求，并将它知道的最近的Leader告知Client节点；
3. 如果Server节点离线，那么RPC连接会超时，回到第一步；
4. 如果Leader节点也离线，那么RPC会超时，回到第一步；
5. 成功连接上Leader节点。

另外，Leader只有将command apply后才回复client。

**优化点**：为了加速client找到leader的速度，client并发向所有节点发送请求。

### Linearizable Semantics 线性化语义 ###

什么是线性化语义：

>  operation appears to execute instantaneously, exactly once, at some point between its invocation and its response
用户的请求操作立即执行，只执行1次，在发起请求和收到回复这段时间内执行。

存在的问题：

**问题1**：Raft可能会重复执行同一条请求。例如，Leader在将请求提交后回复client前宕机，client会向新Leader重发请求。从而导致同一条请求被重复执行。

**解决**：Client为每个请求维护一个唯一的序列号，Server会track每个Client最近几个序列号以及响应。如果server接收到1个已经被执行过的序列号，那么立即回复而不再重复执行。

**问题2**：Client可能读到旧数据。首先，新Leader刚当选后不知道所有已经被提交的日志项。

Leader刚当选后包含所有已提交的entry，但已提交的entry可能没有apply，


Leader刚当选后包含所有已提交的entry，但不知道到哪些entry已apply，哪些还没有apply，所以不能直接返回client状态机里的数据，因为可能是旧数据。
为解决这个问题，Leader当选后先提交一个noop的entry，将所有已经提交的entry提交，apply，然后服务client的读操作。

Leader可能不是最新的，新Leader可能会接收到新的请求更新数据，这个不知道自己已经被隔离的Leader返回给client的数据就会是旧的。所以在返回client读请求前要先通过多数派确认自己依旧是Leader节点。



## 参考资料 ##

1. [寻找一种易于理解的一致性算法（扩展版）](https://github.com/wuxiangwei/raft-zh_cn/blob/master/raft-zh_cn.md)

































